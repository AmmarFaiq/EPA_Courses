{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.MORO OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Generate random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 0.000001) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1000) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = functools.partial(robustness, SMALLER, 150000000)#THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n",
    "model = get_model_for_problem_formulation(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5384ea6185ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mema_logging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_to_stderr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mema_logging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mMultiprocessingEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     results = evaluator.perform_experiments(scenarios=200,               #500\n\u001b[0;32m      7\u001b[0m                                             \u001b[0mpolicies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from ema_workbench import ema_logging, MultiprocessingEvaluator\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=200,               #500\n",
    "                                            policies=4,\n",
    "                                            uncertainty_sampling='mc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8,8),\n",
    "                        sharex=True)\n",
    "axes = [axes[0,0],axes[0,1],axes[1,0],axes[1,1]]                             #axes[1,1]\n",
    "\n",
    "robustness_funcs = {\"Expected Number of Deaths\": Expected_Number_of_Deaths,\n",
    "                    \"Expected Annual Damage\": Expected_Annual_Damage,\n",
    "                    \"Total Investment Costs\": Total_Investment_Costs}\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "for ax, (outcome, value) in zip(axes, outcomes.items()):\n",
    "    for policy in np.unique(experiments['policy']):\n",
    "        logical = experiments['policy'] == policy\n",
    "        data = value[logical]\n",
    "        \n",
    "        robustness = []\n",
    "      \n",
    "        for i in range(1, data.shape[0]):\n",
    "            robustness.append(robustness_funcs[outcome](data[0:i]))\n",
    "        ax.plot(robustness, label=policy)\n",
    "    ax.set_xlabel(\"# experiments\")\n",
    "    ax.set_ylabel(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Search for candidate solutions\n",
    "\n",
    "the fundamental problem is fine tuning the robustness functions. To do this, rather than run optimizaitons many times, why not first generate a test set with a bunch of policies, apply robustness functions and visualize the results?\n",
    "\n",
    "This gives us much faster feedback on reasonble cutoff values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dike_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0900289c9aae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mem_framework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msample_uncertainties\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_scenarios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscenarios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_uncertainties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdike_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_scenarios\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnfe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dike_model' is not defined"
     ]
    }
   ],
   "source": [
    "from ema_workbench.em_framework import sample_uncertainties\n",
    "n_scenarios = 30\n",
    "scenarios = sample_uncertainties(dike_model, n_scenarios)\n",
    "nfe = int(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import (Model, CategoricalParameter,\n",
    "                           ScalarOutcome, IntegerParameter, RealParameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiprocessingEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-718519ada0a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mMultiprocessingEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     results = evaluator.perform_experiments(scenarios=n_scenarios,              \n\u001b[0;32m      3\u001b[0m                                             policies=100)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MultiprocessingEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=n_scenarios,              \n",
    "                                            policies=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'outcomes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-29f3859acfb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutcomes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'policy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'policy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviolinplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Total Investment Costs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'outcomes' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "sns.violinplot(data=data, y='Total Investment Costs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "ax = sns.violinplot(data=data, y='Expected Annual Damage')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "ax = sns.violinplot(data=data, y='Expected Number of Deaths')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import functools\n",
    "\n",
    "# def robustness(direction, threshold, data):\n",
    "#     if direction == SMALLER:\n",
    "#         return np.sum(data<=threshold)/data.shape[0]\n",
    "#     else:\n",
    "#         return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "# SMALLER = 'SMALLER'\n",
    "\n",
    "# Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 1e-5) #not ok\n",
    "# Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1e4) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "# Total_Investment_Costs = functools.partial(robustness, SMALLER, 6e8)#THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from ema_workbench.analysis import parcoords\n",
    "\n",
    "# experiments, outcomes = results\n",
    "\n",
    "# funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "#          'Expected Annual Damage': Expected_Annual_Damage,\n",
    "#          'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "# total_scores = {}\n",
    "# for policy in np.unique(experiments['policy']):\n",
    "#     scores = {}\n",
    "#     logical = experiments['policy'] == policy\n",
    "    \n",
    "#     temp_outcomes = {k:v[logical] for k,v in outcomes.items()}\n",
    "    \n",
    "#     for k, v in temp_outcomes.items():\n",
    "#         score = funcs[k](v)\n",
    "#         scores[k] = score\n",
    "#     total_scores[policy] = scores\n",
    "\n",
    "# data = pd.DataFrame(total_scores).T.reset_index(drop=True)\n",
    "# limits = parcoords.get_limits(data)\n",
    "# limits.loc[0, :] = 0\n",
    "# limits.loc[1, :] = 1\n",
    "\n",
    "# paraxes = parcoords.ParallelAxes(limits)\n",
    "# paraxes.plot(data)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from ema_workbench import (MultiprocessingEvaluator, ema_logging, \n",
    "#                            perform_experiments, SequentialEvaluator)\n",
    "# from ema_workbench.em_framework.optimization import (HyperVolume, \n",
    "#                                                      EpsilonProgress)\n",
    "# from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "\n",
    "# BaseEvaluator.reporting_frequency = 0.1\n",
    "\n",
    "# ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "# # there is a bit of problem with platypus, so using 1.1. gives \n",
    "# # cleaner hypervolume results.\n",
    "# convergence = [HyperVolume(minimum=[0,0,0], maximum=[1.1, 1.1, 1.1]),\n",
    "#               EpsilonProgress()]\n",
    "\n",
    "# epsilons=[0.05,]*len(robustnes_functions)  #final value of epsilon should be much lower.Just for experiment purposes is 1\n",
    "# with MultiprocessingEvaluator(model) as evaluator:\n",
    "#     archive, convergence = evaluator.robust_optimize(robustnes_functions, scenarios,nfe=nfe,\n",
    "#                                                      convergence=convergence, epsilons=epsilons)\n",
    "    \n",
    "# #start = time.time()\n",
    "# #end = time.time()\n",
    "\n",
    "# #print('Processing time:',(end-start)/60,'Minutes')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "# ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "# ax1.set_ylabel('$\\epsilon$-progress')\n",
    "# ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "# ax2.set_ylabel('hypervolume')\n",
    "\n",
    "# ax1.set_xlabel('number of function evaluations')\n",
    "# ax2.set_xlabel('number of function evaluations')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from ema_workbench.analysis import parcoords\n",
    "\n",
    "# data = archive.loc[:, [o.name for o in robustnes_functions]]\n",
    "# limits = parcoords.get_limits(data)\n",
    "# limits.loc[0, :] = 0\n",
    "# limits.loc[1, :] = 1\n",
    "\n",
    "# paraxes = parcoords.ParallelAxes(limits)\n",
    "# paraxes.plot(data)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit better but not much. \n",
    "\n",
    "Now, observe the following: you are using a domain criterion as your sole measure of robustness. That is, you look at the fraction of scenarios above or below a threshold. The costs however do not vary accross scenarios. Thus this objective can only be 0 or 1. This is not particularly useful for optimization. \n",
    "\n",
    "We might thus want to consider another metric for costs. Why not simply use the raw costs itself?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "def costs(data):\n",
    "    return data[0]/1e9 # makes numbers nicer\n",
    "    \n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 1e-5) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1e4) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\tommy\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9db474d46589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparcoords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexperiments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutcomes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "total_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    temp_outcomes = {k:v[logical] for k,v in outcomes.items()}\n",
    "    \n",
    "    for k, v in temp_outcomes.items():\n",
    "        score = funcs[k](v)\n",
    "        scores[k] = score\n",
    "    total_scores[policy] = scores\n",
    "\n",
    "data = pd.DataFrame(total_scores).T.reset_index(drop=True)\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks much nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAXIMIZE = ScalarOutcome.MAXIMIZE\n",
    "MINIMIZE = ScalarOutcome.MINIMIZE\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "robustnes_functions = [ScalarOutcome('fraction max_p', kind=MAXIMIZE, \n",
    "                             variable_name='Expected Number of Deaths', function=Expected_Number_of_Deaths),\n",
    "                       ScalarOutcome('fraction reliability', kind=MAXIMIZE, \n",
    "                             variable_name='Expected Annual Damage', function=Expected_Annual_Damage),\n",
    "                       ScalarOutcome('fraction inertia', kind=MINIMIZE, # note that we have to minimize costs!\n",
    "                             variable_name='Total Investment Costs', function=Total_Investment_Costs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c5365c8ab2e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrobustnes_functions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#final value of epsilon should be much lower.Just for experiment purposes is 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mMultiprocessingEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     archive, convergence = evaluator.robust_optimize(robustnes_functions, scenarios,nfe=nfe,\n\u001b[0;32m     17\u001b[0m                                                      convergence=convergence, epsilons=epsilons)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# we have to change the plausible max for total investment costs\n",
    "from ema_workbench import (MultiprocessingEvaluator, ema_logging, \n",
    "                            perform_experiments, SequentialEvaluator)\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume, \n",
    "                                                      EpsilonProgress)\n",
    "from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "\n",
    "BaseEvaluator.reporting_frequency = 0.1\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "convergence = [HyperVolume(minimum=[0,0,0], maximum=[1.1, 1.1, 3]),\n",
    "              EpsilonProgress()]\n",
    "\n",
    "epsilons=[0.1,]*len(robustnes_functions)  #final value of epsilon should be much lower.Just for experiment purposes is 1\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    archive, convergence = evaluator.robust_optimize(robustnes_functions, scenarios,nfe=nfe,\n",
    "                                                     convergence=convergence, epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = archive.loc[:, [o.name for o in robustnes_functions]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Re-evaluate candidate solutions under uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import Policy\n",
    "\n",
    "policies = archive.drop([o.name for o in robustnes_functions], axis=1)\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(\"moro {}\".format(i), **policy.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_scenarios = 1000\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)\n",
    "\n",
    "#start = time.time()\n",
    "#end = time.time()\n",
    "\n",
    "#print('Processing time:',(end-start)/60,'Minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import save_results\n",
    "\n",
    "save_results(results, 'MORO_reevaluation.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policies.to_csv('moro polices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "experiments, outcomes = results\n",
    "\n",
    "overall_robustness = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    policy_robustness = {}\n",
    "\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    for outcome, values in outcomes.items():\n",
    "        values = values[logical]\n",
    "        policy_robustness[outcome] = robustness_funcs[outcome](values)\n",
    "    overall_robustness[policy] = policy_robustness\n",
    "overall_robustness = pd.DataFrame.from_dict(overall_robustness).T\n",
    "overall_robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = overall_robustness.loc[:, \n",
    "                              ['Expected Number of Deaths', 'Expected Annual Damage', 'Total Investment Costs']]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
